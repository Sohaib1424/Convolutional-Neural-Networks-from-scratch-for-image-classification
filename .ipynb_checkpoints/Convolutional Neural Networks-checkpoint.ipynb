{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "In Deep Learning, a _Convolutional Neural Network_ (CNN, or ConvNet) is a class of Artificial Neural Network, most commonly applied to analyze visual imagery. This type of Artificial Neural network has applications in __image and video recognition__, __image classification__, __image segmentation__, __medical image analysis__, and etc.\n",
    "\n",
    "The name _`Convolutional Neural Network`_ indicates that the network employs a mathematical operation called __Convolution__. CNNs are a specialized type of Neural Networks that use Convolution in place of general matrix multiplication in at least one of their layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning goals\n",
    "- Learning the structure of simple Convolutional Neural Network\n",
    "- Learning about Convolution Layers and Pooling Layers and their roles in a Convolutional Neural Network\n",
    "- Implementing simple Convolutional Neural Network from scratch using scipy and numpy and training a model for __image classification__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [Stanford CS231n](https://www.youtube.com/watch?v=NfnWJUyUJYU&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend you to check my previous notebook on __[implementing simple Neural Networks from scratch](https://github.com/Sohaib1424/Neural-Networks-from-scratch)__ before you start reading this notebook. Some of the concepts like preprocessing training data, the _Softmax Cross Entropy Loss_, etc, are explianed in the mentioned notebook.\n",
    "\n",
    "\n",
    "#### Convolutional Neural Networks for Image Classification\n",
    "\n",
    "A Convolutional Neural Network is a combination of two components: the __Feature Learning__ (or __Feature Extraction__) part and the __Classification__ part. The feature learning part is where the Convolution Layers and Pooling Layers are. They are responsible for learning (or extracting) features and downsampling the results. Having at least one Convolution Layer in this part is necessary, but having no Pooling Layers is totally fine. The Classification part is only consisted of Fully-Connected Layers.\n",
    "\n",
    "The input to this type of Neural Network has to be three dimentional ($W\\times{H}\\times{D}$), but during feedforward at the last layer in the feature learning part, the ouput of that layer gets flattened to a one dimentional vector and passed to the classification part, and during backprop the one dimentional derivative vector is reshaped back to $W\\times{H}\\times{D}$.\n",
    "\n",
    "<img src=\"imgs\\convNet.jpg\" height=\"72%\" width=\"72%\" style=\"float:center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(X, y, val_size=.2, test_size=.2, shuffle=False):\n",
    "    if val_size < 0 or test_size < 0:\n",
    "        \n",
    "        raise EnvironmentError(\"validation or test size should not be negative!\")\n",
    "    \n",
    "    if val_size + test_size >= 1.:\n",
    "        raise EnvironmentError(\"validation + test size should be lesser than 1!\")\n",
    "    \n",
    "    x_data, y_data = X, y\n",
    "    if shuffle:\n",
    "        for _ in range(3):\n",
    "            temp = list(zip(x_data.T, y_data.T))\n",
    "            random.shuffle(temp)\n",
    "            x_data, y_data = zip(*temp)\n",
    "            x_data = np.array(list(x_data)).T\n",
    "            y_data = np.array(list(y_data)).T\n",
    "        \n",
    "    \n",
    "    val_indx = int(val_size * x_data.shape[1])\n",
    "    test_indx = int(test_size * x_data.shape[1])\n",
    "    X_val, X_test, X_train = x_data[:, :val_indx], x_data[:, val_indx:val_indx+test_indx], x_data[:, val_indx+test_indx:]\n",
    "    y_val, y_test, y_train = y_data[:, :val_indx], y_data[:, val_indx:val_indx+test_indx], y_data[:, val_indx+test_indx:]\n",
    "    \n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    return 100. * sum(np.argmax(y_pred, axis=0) == np.argmax(y_true, axis=0)) / y_true.shape[1]\n",
    "\n",
    "# A function to save a trained model\n",
    "def saveModel(model, name):\n",
    "    \n",
    "    if type(model) != ConvolutionNeuralNetwork:\n",
    "        raise EnvironmentError(\"Wrong model type!!!\")\n",
    "    \n",
    "    if type(name) != str:\n",
    "        raise EnvironmentError(\"`name` should be of type string!!!\")\n",
    "    \n",
    "    modelInfo = {\n",
    "            \"Layers\" : [],\n",
    "            \"optMethod\" : model.optMethod,\n",
    "            \"alpha\" : model.alpha,\n",
    "            \"momentum\": model.momentum,\n",
    "            \"iterations\" : model.num_iterations,\n",
    "            \"decay_rate\": model.decay_rate,\n",
    "            \"costFunc\" : model.costFunc,\n",
    "    }\n",
    "    \n",
    "    c, p, f = 0, 0, 0\n",
    "    \n",
    "    for i in range(len(model.Layers)):\n",
    "        if type(model.Layers[i]) == convLayer:\n",
    "            c += 1\n",
    "            modelInfo[\"Layers\"].append(f\"CL{c}\")\n",
    "            modelInfo[f\"CL{c}\"] = {\n",
    "                                       \"type\" : \"CL\",\n",
    "                                       \"kernels\" : model.Layers[i].kernels,\n",
    "                                       \"bias\" : model.Layers[i].bias,\n",
    "                                       \"activation\" : model.Layers[i].activation,\n",
    "                                       \"widthStride\" : model.Layers[i].Wstride,\n",
    "                                       \"heightStride\" : model.Layers[i].Hstride,\n",
    "                                       \"padWidth\" : model.Layers[i].padW,\n",
    "                                       \"padHeight\" : model.Layers[i].padH\n",
    "                                    }\n",
    "            \n",
    "        elif type(model.Layers[i]) == poolLayer:\n",
    "            p += 1\n",
    "            modelInfo[\"Layers\"].append(f\"PL{p}\")\n",
    "            modelInfo[f\"PL{p}\"] = {\n",
    "                                       \"type\" : \"PL\",\n",
    "                                       \"poolSize\" : model.Layers[i].poolSize,\n",
    "                                       \"H_stepSize\" : model.Layers[i].H_stepSize,\n",
    "                                       \"V_stepSize_stepSize\" : model.Layers[i].V_stepSize,\n",
    "                                       \"method\" : model.Layers[i].method,        \n",
    "                                    }\n",
    "            \n",
    "        elif type(model.Layers[i]) == FCLayer:\n",
    "            f += 1\n",
    "            modelInfo[\"Layers\"].append(f\"FC{f}\")\n",
    "            modelInfo[f\"FC{f}\"] = {\n",
    "                                       \"type\" : \"FC\",\n",
    "                                       \"weights\" : model.Layers[i].W,\n",
    "                                       \"bias\" : model.Layers[i].b,\n",
    "                                       \"Vw\" : model.Layers[i].Vw,\n",
    "                                       \"Vb\" : model.Layers[i].Vb,\n",
    "                                       \"W_cache\" : model.Layers[i].W_cache,\n",
    "                                       \"b_cache\" : model.Layers[i].b_cache,\n",
    "                                       \"Activation\" : model.Layers[i].activation,        \n",
    "                                    }\n",
    "    \n",
    "    if not name.__contains__(\".pickle\"):\n",
    "        pickle.dump(modelInfo, open(f\"{name}.pickle\", \"wb\"))\n",
    "    else:\n",
    "        pickle.dump(modelInfo, open(f\"{name}\", \"wb\"))\n",
    "    \n",
    "    print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rectified linear unit\n",
    "def ReLU(x):\n",
    "    x = np.where(x < 0, 0, x)\n",
    "    return x\n",
    "# derivative of ReLU\n",
    "def dReLU(x):\n",
    "    x = np.where(x < 0, 0, x)\n",
    "    x = np.where(x > 0, 1, x)\n",
    "    return x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1. - tanh(x)**2\n",
    "\n",
    "# sigmoid function\n",
    "def sigm(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid function\n",
    "def dsigm(x):\n",
    "    z = sigm(x)\n",
    "    return z * (1. - z)\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=0, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    exp_x /= np.sum(exp_x, axis=0, keepdims=True)\n",
    "    return exp_x\n",
    "def dsoftmax(y_hat, y):\n",
    "    return y_hat - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Methods\n",
    "\n",
    "In this notebook you'll see how `GD with Momentum` works. For more info on other optimization methods vist these links: [AdaGrad](https://www.geeksforgeeks.org/intuition-behind-adagrad-optimizer/),  [RMSProp](https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b),  [Adam](https://www.geeksforgeeks.org/intuition-of-adam-optimizer/)\n",
    "\n",
    "Assuming you know how GD works, let's jump into adding momentum to it. The problem of GD is that it doesn't keep any memory of the past, and hence it may zigzag a lot during optimization trying to reach the optimum point. The image below illustrates this:\n",
    "\n",
    "<img src=\"imgs/sgd.png\" width=\"75%\" height=\"75%\" style=\"float:center\">\n",
    "\n",
    "Let's add memory by defining a variable like `V`. With each iteration of optimization, we store its derivative in `V`. As the algorithm progresses down the line, the derivatives from older iterations have to be less influential on the direction in which the algorithm walks because they've already had their impact on that path, and newer derivatives need to have more effect and have more importance. Thus, the mathematical formula for optimizing weights and biases will be:\n",
    "\n",
    "- Memory for optimizing weights: \n",
    "$$V_w^k = \\beta{V_w^{k-1}}+\\alpha\\frac{\\partial\\mathscr{L}_k}{\\partial{W}}, \\ \\ k \\geq 1\\tag{1}$$\n",
    "\n",
    "- Memory for optimizing biases: \n",
    "$$V_b^k = \\beta{V_b^{k-1}}+\\alpha\\frac{\\partial\\mathscr{L}_k}{\\partial{b}}, \\ \\ k \\geq 1\\tag{2}$$\n",
    "\n",
    "And if we unfold these formulas we'll have:\n",
    "\n",
    "- Memory for optimizing weights: \n",
    "$$V_w^k = \\beta^2{V_w^{k-2}}+\\beta\\alpha\\frac{\\partial\\mathscr{L}_{k-1}}{\\partial{W}}+\\alpha\\frac{\\partial\\mathscr{L}_k}{\\partial{W}} \\\\\n",
    "V_w^k = \\beta^3{V_w^{k-3}}+\\beta^2\\alpha\\frac{\\partial\\mathscr{L}_{k-2}}{\\partial{W}}+\\beta\\alpha\\frac{\\partial\\mathscr{L}_{k-1}}{\\partial{W}}+\\alpha\\frac{\\partial\\mathscr{L}_k}{\\partial{W}}\\\\\n",
    "\\vdots \\\\\n",
    "V_w^k = \\alpha\\sum_{i = 0}^{k} \\beta^{k-i}\\frac{\\partial\\mathscr{L}_i}{\\partial{W}}\n",
    "$$ $\\tag{3}$\n",
    "\n",
    "\n",
    "\n",
    "- Memory for optimizing biases: \n",
    "$$V_b^k = \\beta^2{V_b^{k-2}}+\\beta\\alpha\\frac{\\partial\\mathscr{L}_{k-1}}{\\partial{b}}+\\alpha\\frac{\\partial\\mathscr{L}_k}{\\partial{b}} \\\\\n",
    "V_b^k = \\beta^3{V_b^{k-3}}+\\beta^2\\alpha\\frac{\\partial\\mathscr{L}_{k-2}}{\\partial{b}}+\\beta\\alpha\\frac{\\partial\\mathscr{L}_{k-1}}{\\partial{b}}+\\alpha\\frac{\\partial\\mathscr{L}_k}{\\partial{b}}\\\\\n",
    "\\vdots \\\\\n",
    "V_b^k = \\alpha\\sum_{i = 0}^{k} \\beta^{k-i}\\frac{\\partial\\mathscr{L}_i}{\\partial{b}}\n",
    "$$ $\\tag{4}$\n",
    "\n",
    "As in Eq3 and Eq4, the derivatives from older steps have less effect on the direction the algorithm walks. With this, the algorithm walks a smoother path instead of zigzagging, and it will take lesser steps (iterations) to reach the optimum values. The image below illustrates this perfectly:\n",
    "\n",
    "<img src=\"imgs/momentum.png\" width=\"75%\" height=\"75%\" style=\"float:center\">\n",
    "\n",
    "For more on why momentum really works, visit [here](https://distill.pub/2017/momentum/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "def GD(alpha, E, d):\n",
    "    return E - alpha * d\n",
    "\n",
    "# Gradient Descent with `Momentum`\n",
    "def Momentum(mu, alpha, V, E, d):\n",
    "    V = mu * V - alpha * d\n",
    "    E += V\n",
    "    return V, E\n",
    "\n",
    "# Adaptive Gradient\n",
    "def Adagrad(alpha, E, dE, cache):\n",
    "    cache += dE**2\n",
    "    E -= alpha * dE / np.sqrt(cache + 1e-9)\n",
    "    \n",
    "    return cache, E\n",
    "\n",
    "# Root Mean Squared Propagation\n",
    "def RMSProp(alpha, decay_rate, E, dE, cache):\n",
    "    cache = decay_rate * cache + (1. - decay_rate) * dE**2\n",
    "    E -= alpha * dE / np.sqrt(cache + 1e-9)\n",
    "    \n",
    "    return cache, E\n",
    "\n",
    "# Adaptive Gradient with `Momentum`\n",
    "def Adam(alpha, decay_rate, momentum, V, E, dE, cache):\n",
    "    V = momentum * V + (1. - momentum) * dE\n",
    "    cache = decay_rate * cache + (1. - decay_rate) * dE**2\n",
    "    E -= alpha * V / np.sqrt(cache + 1e-9)\n",
    "    return E, V, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_cost(y_hat, y):\n",
    "    cost = np.mean(-np.sum(y * np.log(y_hat+1e-9), axis=0))\n",
    "    return cost\n",
    "\n",
    "def dsoftmax_cross_entropy_cost(y_hat, y):\n",
    "    # equation 19: derivative of Loss with respect to z\n",
    "    return dsoftmax(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"./Data/train.csv\"\n",
    "\n",
    "data_train = pd.read_csv(path)\n",
    "\n",
    "x = np.array(data_train.drop(columns=[\"label\"])).T /256.\n",
    "y = np.zeros((10, 42000))\n",
    "\n",
    "for i in range(42000):\n",
    "    y[data_train.iloc[i][\"label\"], i] = 1.\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(x[:, random.randint(0, 41999)].reshape(28, 28), interpolation='nearest', cmap=plt.cm.Greys)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Random sample!\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = train_valid_test_split(x, y, test_size=.2, val_size=.15, shuffle=True)\n",
    "# mean value of features\n",
    "mu = np.mean(X_train, axis=1, keepdims=True)\n",
    "\n",
    "# remove mean vector from all data\n",
    "X_train -= mu\n",
    "X_val -= mu\n",
    "X_test  -= mu\n",
    "\n",
    "X_train = X_train.T.reshape(X_train.shape[1], 28, 28, 1)\n",
    "X_val = X_val.T.reshape(X_val.shape[1], 28, 28, 1)\n",
    "X_test = X_test.T.reshape(X_test.shape[1], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer\n",
    "\n",
    "The following class is a slightly modified version of the `layer` class in the mentioned notebook above. Explanation on how fully-connected layers function and how to optimize for their weights and biases can be found there. Thus, I avoided explaining them again in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following class allows us to create fully connected layers with varying neuron counts\n",
    "class FCLayer:\n",
    "    \n",
    "    def __init__(self, n_neurons, Activation=\"ReLU\"):\n",
    "        if n_neurons <= 0:\n",
    "            raise EnvironmentError(\"Input values should be positive!\")\n",
    "        \n",
    "        self.n_neurons = n_neurons\n",
    "        self.activation = Activation\n",
    "        \n",
    "        self.x = None # the given input to the Layer        \n",
    "        self.z = None # the result from the linear part (Wx + b)\n",
    "        self.a = None # the result from the activation part (activation(z))\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.Vw = 0\n",
    "        self.Vb = 0\n",
    "        self.W_cache = 0\n",
    "        self.b_cache = 0\n",
    "        \n",
    "        \n",
    "    def activateLayer(self, in_shape):\n",
    "        self.shape = np.array([self.n_neurons, in_shape])\n",
    "        \n",
    "        # Initializing Wieghts and specifying activation function for the layer\n",
    "        if self.activation == \"ReLU\":\n",
    "            self.Activation = ReLU\n",
    "            self.dActivation = dReLU\n",
    "            if self.W is None:\n",
    "                self.W = np.random.randn(*self.shape) * np.sqrt(2. / self.shape[1])\n",
    "        elif self.activation == \"sigm\":\n",
    "            self.Activation = sigm\n",
    "            self.dActivation = dsigm\n",
    "            if self.W is None:\n",
    "                self.W = np.random.randn(*self.shape) * np.sqrt(1. / self.shape[1])\n",
    "        elif self.activation == \"tanh\":\n",
    "            self.Activation = tanh\n",
    "            self.dActivation = dtanh\n",
    "            if self.W is None:\n",
    "                self.W = np.random.randn(*self.shape) * np.sqrt(1. / sum(self.shape))\n",
    "        elif self.activation == \"Linear\":\n",
    "            self.Activation = lambda x: x\n",
    "            self.dActivation = lambda x: 1.\n",
    "            if self.W is None:\n",
    "                self.W = np.random.randn(*self.shape)\n",
    "        elif self.activation == \"soft\":\n",
    "            self.Activation = softmax\n",
    "            self.dActivation = dsoftmax\n",
    "            if self.W is None:\n",
    "                self.W = np.random.randn(*self.shape) * 0.001\n",
    "        else:\n",
    "            raise EnvironmentError(\"Activation function not defined!\")\n",
    "            \n",
    "        if self.b is None:\n",
    "            self.b = np.zeros((self.shape[0], 1))\n",
    "        \n",
    "    def train(self, x):\n",
    "        self.x = x\n",
    "        self.z = self.W @ self.x + self.b # computing the linear part\n",
    "        self.a = self.Activation(self.z) # computing the activation part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer\n",
    "\n",
    "The first layer in every Convolutional Neural Network is a _Convolution Layer_. Convolution Layers are the backbones of the Convolutional Neural Network. They enable the network to extract features via filters (Kernels) from input data that might help achieve better performance. Convolution Layers have two parameters, the kernels, and the biases. A Convolution Layer can have as many kernels (filters) as desired, and the depth of each kernel is the same as the depth of the layer's input. For example, if the input data is a colored image, then it has a depth of three because of the RGB channels. Thus each kernel will have a depth of three, and the layer's output will have a depth equal to the number of kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Correlation\n",
    "\n",
    "The operation that enables the network to extract various features from input data is called __Cross-Correlation__. Cross-Correlation takes a kernel and slides it along the width and the height of the input data, taking an elementwise dot product of the kernel and the covered part of the input, then summing up all the elements and storing the result in layer's output, just like in the picture and the GIF represented below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs\\cross-correlation.png\" width=\"75%\" height=\"75%\" style=\"float:center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs\\conv_op.gif\" width='30%' height='30%' style=\"float:center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at the end we add a bias term to the operation's output and apply an activation function like `ReLU`, and then forward the final result to the next layer.\n",
    "\n",
    "So, the mathematical formula for all above, with one input and one kernel, would be:\n",
    "$$ O = I * K + b \\tag{1}$$\n",
    "If there are multiple inputs and multiple kernels then:\n",
    "$$ O_{jl} = I_{j} * K_{l} + b_{l} \\ \\ \\ \\forall j \\in {1,...,n} \\ \\ , \\forall l \\in {1,...,k}\\tag{2}$$\n",
    "\n",
    "Let's discus about the output of a Convolution Layer. First of all, its depth is equal to the number of kernels in that layer, and its height and width are calculated as followed:\n",
    "\n",
    "- Output width: $W_{out} = W_{in} - W_{kernel} + 1$\n",
    "- Output height: $H_{out} = H_{in} - H_{kernel} + 1$\n",
    "\n",
    "This is the simplest case. But what if we want the output to be of a specific shape? We know how to change the depth of it as desired easily by changing the number of filters, but for the height and the width we need other parameters.\n",
    "\n",
    "If we want $W_{out}$ or $H_{out}$ to be bigger than $W_{in} - W_{kernel} + 1$ or $H_{in} - H_{kernel} + 1$ respectively, we make them bigger by adding zero-paddings, just like the GIF above. The gray squares at the buttom are zero-paddings and the purple squares at the buttom are the actual input data, and with the help of zero-padding, the output has the same height and width as the input.\n",
    "\n",
    "Now, what if we want $W_{out}$ or $H_{out}$ to be smaller than $W_{in} - W_{kernel} + 1$ or $H_{in} - H_{kernel} + 1$ respectively? We can do that by sliding a kernel __multiple__ pixels at a time instead of one pixel along the width or the height:\n",
    "\n",
    "<img src=\"imgs\\stride.gif\" height=\"30%\" width=\"30%\" style=\"float:center\">\n",
    "\n",
    "Thus, the new formulas for calculating $W_{out}$ and $H_{out}$ are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "W_{out} = \\lfloor\\frac{W_{in} - W_{kernel} + 2\\times{W_{pad}}}{W_{stride}}\\rfloor + 1\\tag{3} \\\\\\\\\n",
    "H_{out} = \\lfloor\\frac{H_{in} - H_{kernel} + 2\\times{H_{pad}}}{H_{stride}}\\rfloor + 1\\tag{4}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for convolving two functions (signals) found in math books has two differences from Eq1 and Eq2. (In\n",
    "math books, the convolution of two continuous functions (signals) $f$ and $g$ is $f(t)*g(t) = \\int_{-\\infty}^{+ \\infty} \n",
    "f(\\tau)g(t-\\tau)\\partial{\\tau} \\ \\ $, or two discrete signals $f$ and $g$ is $f[n]*g[n] = \\sum_{k=-\\infty}^{+\\infty} f[k]g[k-n]$). The differences are: __1__- one of the signals is rotated 180 degrees (reflected signal $g[k-n]$) __2__- the operation used here is full Cross-Correlation. Thus, don't confuse the operator `*` in Eq1 and Eq2 with the formulas found in math books. The GIF below illustrates full Cross-Correlation:\n",
    "\n",
    "\n",
    "<img src=\"imgs\\full_cr.gif\" height=\"30%\" width=\"30%\" style=\"float:center\">\n",
    "\n",
    "- Note: The ouput of a Convolution Layer is also called `feature maps` or `activation maps` of that layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "For simplicity, consider the example below where we only have one input with one channel and only one kernel:\n",
    "\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "y_{11} & y_{12}\\\\\n",
    "y_{21} & y_{22}\n",
    "\\end{bmatrix} = X * K + b = \n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13}\\\\\n",
    "x_{21} & x_{22} & x_{23}\\\\\n",
    "x_{31} & x_{32} & x_{23}\\\\\n",
    "\\end{bmatrix} * \n",
    "\\begin{bmatrix}\n",
    "k_{11} & k_{12}\\\\\n",
    "k_{21} & k_{22}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12}\\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix}\\tag{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{11} = x_{11}k_{11} + x_{12}k_{12} + x_{21}k_{21} + x_{22}k_{22} + b_{11} \\\\\n",
    "y_{12} = x_{12}k_{11} + x_{13}k_{12} + x_{22}k_{21} + x_{23}k_{22} + b_{12} \\\\\n",
    "y_{21} = x_{21}k_{11} + x_{22}k_{12} + x_{31}k_{21} + x_{32}k_{22} + b_{21} \\\\\n",
    "y_{22} = x_{22}k_{11} + x_{23}k_{12} + x_{32}k_{21} + x_{33}k_{22} + b_{22} \\\\\n",
    "$$\n",
    "\n",
    "And the given derivative matrix (derivative with respect to $Y$) is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{Y}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{E}}{\\partial{Y}_{11}} & \\frac{\\partial{E}}{\\partial{Y}_{12}}\\\\\n",
    "\\frac{\\partial{E}}{\\partial{Y}_{21}} & \\frac{\\partial{E}}{\\partial{Y}_{22}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Calculating $\\frac{\\partial{E}}{\\partial{K}}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{K}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{E}}{\\partial{k_{11}}} & \\frac{\\partial{E}}{\\partial{k_{12}}}\\\\\n",
    "\\frac{\\partial{E}}{\\partial{k_{21}}} & \\frac{\\partial{E}}{\\partial{k_{22}}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{k_{11}}} = \\frac{\\partial{E}}{\\partial{y_{11}}}\\frac{\\partial{y_{11}}}{\\partial{k_{11}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{12}}}\\frac{\\partial{y_{12}}}{\\partial{k_{11}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{21}}}\\frac{\\partial{y_{21}}}{\\partial{k_{11}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{22}}}\\frac{\\partial{y_{22}}}{\\partial{k_{11}}} = \n",
    "\\frac{\\partial{E}}{\\partial{y_{11}}}x_{11} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{12}}}x_{12} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{21}}}x_{21} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{22}}}x_{22}\n",
    "\\\\ \\ \\\\ \\ \\\\\n",
    "\\frac{\\partial{E}}{\\partial{k_{12}}} = \\frac{\\partial{E}}{\\partial{y_{11}}}\\frac{\\partial{y_{11}}}{\\partial{k_{12}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{12}}}\\frac{\\partial{y_{12}}}{\\partial{k_{12}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{21}}}\\frac{\\partial{y_{21}}}{\\partial{k_{12}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{22}}}\\frac{\\partial{y_{22}}}{\\partial{k_{12}}} = \n",
    "\\frac{\\partial{E}}{\\partial{y_{11}}}x_{12} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{12}}}x_{13} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{21}}}x_{22} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{22}}}x_{23}\n",
    "\\\\ \\ \\\\ \\ \\\\\n",
    "\\frac{\\partial{E}}{\\partial{k_{21}}} = \\frac{\\partial{E}}{\\partial{y_{11}}}\\frac{\\partial{y_{11}}}{\\partial{k_{21}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{12}}}\\frac{\\partial{y_{12}}}{\\partial{k_{21}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{21}}}\\frac{\\partial{y_{21}}}{\\partial{k_{21}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{22}}}\\frac{\\partial{y_{22}}}{\\partial{k_{21}}} = \n",
    "\\frac{\\partial{E}}{\\partial{y_{11}}}x_{21} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{12}}}x_{22} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{21}}}x_{31} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{22}}}x_{32}\n",
    "\\\\ \\ \\\\ \\ \\\\\n",
    "\\frac{\\partial{E}}{\\partial{k_{22}}} = \\frac{\\partial{E}}{\\partial{y_{11}}}\\frac{\\partial{y_{11}}}{\\partial{k_{22}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{12}}}\\frac{\\partial{y_{12}}}{\\partial{k_{22}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{21}}}\\frac{\\partial{y_{21}}}{\\partial{k_{22}}} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{22}}}\\frac{\\partial{y_{22}}}{\\partial{k_{22}}} = \n",
    "\\frac{\\partial{E}}{\\partial{y_{11}}}x_{22} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{12}}}x_{23} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{21}}}x_{32} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{22}}}x_{33}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the equations above are: \n",
    "$$\\frac{\\partial{E}}{\\partial{K}} = \n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13}\\\\\n",
    "x_{21} & x_{22} & x_{23}\\\\\n",
    "x_{31} & x_{32} & x_{23}\\\\\n",
    "\\end{bmatrix} * \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{E}}{\\partial{Y}_{11}} & \\frac{\\partial{E}}{\\partial{Y}_{12}}\\\\\n",
    "\\frac{\\partial{E}}{\\partial{Y}_{21}} & \\frac{\\partial{E}}{\\partial{Y}_{22}}\n",
    "\\end{bmatrix} = X * \\frac{\\partial{E}}{\\partial{Y}}\\tag{6}\n",
    "$$\n",
    "\n",
    "- Calculating $\\frac{\\partial{E}}{\\partial{b}}$:\n",
    "\n",
    "By repeating the same steps as before, we'll have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{b}} = \\frac{\\partial{E}}{\\partial{Y}}\\tag{7}\n",
    "$$\n",
    "\n",
    "- Calculating $\\frac{\\partial{E}}{\\partial{X}}$:\n",
    "\n",
    "We need to calculate $\\frac{\\partial{E}}{\\partial{X}}$ and pass it to previous layer as its derivative matrix, but keep in mind that $\\frac{\\partial{E}}{\\partial{X}}$ should have the same shape as the previous layer's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{X}} = \\begin{bmatrix}\n",
    "\\frac{\\partial{E}}{\\partial{x_{11}}} & \\frac{\\partial{E}}{\\partial{x_{12}}} & \\frac{\\partial{E}}{\\partial{x_{13}}}\\\\\n",
    "\\frac{\\partial{E}}{\\partial{x_{21}}} & \\frac{\\partial{E}}{\\partial{x_{22}}} & \\frac{\\partial{E}}{\\partial{x_{23}}}\\\\\n",
    "\\frac{\\partial{E}}{\\partial{x_{31}}} & \\frac{\\partial{E}}{\\partial{x_{32}}} & \\frac{\\partial{E}}{\\partial{x_{33}}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial{E}}{\\partial{x_{11}}} =& \\frac{\\partial{E}}{\\partial{y_{11}}}k_{11}\n",
    "\\\\ \\ \\\\\n",
    "\\frac{\\partial{E}}{\\partial{x_{11}}} =& \\frac{\\partial{E}}{\\partial{y_{11}}}k_{12} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{12}}}k_{11}\n",
    "\\\\ \\ \\\\\n",
    "\\frac{\\partial{E}}{\\partial{x_{11}}} =& \\frac{\\partial{E}}{\\partial{y_{12}}}k_{12}\n",
    "\\\\ \\ \\\\\n",
    "\\frac{\\partial{E}}{\\partial{x_{11}}} =& \\frac{\\partial{E}}{\\partial{y_{11}}}k_{21} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{21}}}k_{11}\n",
    "\\\\ \\ \\\\\n",
    "\\frac{\\partial{E}}{\\partial{x_{11}}} =& \\frac{\\partial{E}}{\\partial{y_{11}}}k_{22} +\n",
    "\\frac{\\partial{E}}{\\partial{y_{12}}}k_{21} + \\frac{\\partial{E}}{\\partial{y_{21}}}k_{12} +\n",
    "\\frac{\\partial{E}}{\\partial{y_{22}}}k_{11}\n",
    "\\\\ \\ \\\\\n",
    "\\frac{\\partial{E}}{\\partial{x_{11}}} =& \\frac{\\partial{E}}{\\partial{y_{12}}}k_{22} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{22}}}k_{12}\n",
    "\\\\ \\ \\\\\n",
    "\\frac{\\partial{E}}{\\partial{x_{11}}} =& \\frac{\\partial{E}}{\\partial{y_{21}}}k_{21}\n",
    "\\\\ \\ \\\\\n",
    "\\frac{\\partial{E}}{\\partial{x_{11}}} =& \\frac{\\partial{E}}{\\partial{y_{21}}}k_{22} + \n",
    "\\frac{\\partial{E}}{\\partial{y_{22}}}k_{21}\n",
    "\\\\ \\ \\\\\n",
    "\\frac{\\partial{E}}{\\partial{x_{11}}} =& \\frac{\\partial{E}}{\\partial{y_{22}}}k_{22}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look closely, those equations above are resulted from convolving $\\frac{\\partial{E}}{\\partial{Y}}$ with $K$. In other words, we perform full Cross-Correlation on $\\frac{\\partial{E}}{\\partial{Y}}$ and $rotate_{180}(K)$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{X}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{E}}{\\partial{Y}_{11}} & \\frac{\\partial{E}}{\\partial{Y}_{12}}\\\\\n",
    "\\frac{\\partial{E}}{\\partial{Y}_{21}} & \\frac{\\partial{E}}{\\partial{Y}_{22}}\n",
    "\\end{bmatrix} *_{_{full}} \n",
    "\\begin{bmatrix}\n",
    "k_{22} & k_{21}\\\\\n",
    "k_{12} & k_{11}\n",
    "\\end{bmatrix}\\tag{8}\n",
    "$$\n",
    "\n",
    "One problem remains. During feedforward, when the layer adds zero-paddings to its input data of shape $W\\times{H}\\times{D}$, the input shape changes to $W_{new}\\times{H_{new}}\\times{D}$, and during backprop $\\frac{\\partial{E}}{\\partial{X}}$ will be of shape $W_{new}\\times{H_{new}}\\times{D}$. How should we deal with this problem? Simple. We just cut the extra parts corresponding to the zero-paddings.\n",
    "\n",
    "Eq6, Eq7 and Eq8 can be generalized easily to a situation where there are multiple inputs with multiple channels and multiple kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convLayer:\n",
    "    \n",
    "    def __init__(self, kernel_width=3, kernel_height=3, depth=6, width_stride=1, \n",
    "                 height_stride=1, pad_width=0, pad_height=0, activation=\"ReLU\"):\n",
    "        \n",
    "        if depth % 1 != 0:\n",
    "            raise EnvironmentError(\"Depth value should not be a float number!!!\")\n",
    "            \n",
    "        if kernel_height % 1 != 0 or kernel_width % 1 != 0:\n",
    "            raise EnvironmentError(\"Kernel size should not be a float number!!!\")\n",
    "            \n",
    "        if width_stride % 1 != 0 or height_stride % 1 != 0:\n",
    "            raise EnvironmentError(\"Stride should not be a float number!!!\")\n",
    "            \n",
    "        if width_stride < 1 or height_stride < 1:\n",
    "            raise EnvironmentError(\"Stride should be a positive number!!!\")\n",
    "        \n",
    "        self.kW = kernel_width\n",
    "        self.kH = kernel_height\n",
    "        self.depth = depth\n",
    "        self.Wstride = width_stride\n",
    "        self.Hstride = height_stride\n",
    "        self.padW = pad_width\n",
    "        self.padH = pad_height\n",
    "        \n",
    "        if activation == \"ReLU\":\n",
    "            self.Activation = lambda x: np.maximum(x, 0)\n",
    "        else:\n",
    "            raise EnvironmentError(\"Activation function for convLayer not defined!!!\")\n",
    "            \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.kernels = None\n",
    "        self.bias = None\n",
    "        self.Vk = 0\n",
    "        self.Vb = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n, W, H, D = x.shape\n",
    "        \n",
    "        self.x = np.zeros((n, W + 2*self.padW, H + 2*self.padH, D))\n",
    "        self.x[:, self.padW: W+self.padW, self.padH: H+self.padH, :] = x  \n",
    "        \n",
    "        if self.kernels is None:\n",
    "            self.kernels = 2 * np.random.random(size=(self.depth, self.kW, self.kH, D)) - 1.\n",
    "        \n",
    "            self.W_out = int(np.floor((W + 2*self.padW - self.kW) / self.Wstride + 1))\n",
    "            self.H_out = int(np.floor((H + 2*self.padH - self.kH) / self.Hstride + 1))\n",
    "            self.D_out = self.depth\n",
    "        \n",
    "            self.bias = np.zeros((1, self.W_out, self.H_out, self.D_out))\n",
    "        \n",
    "        self.res = np.zeros((n, self.W_out, self.H_out, self.D_out))\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(self.D_out):\n",
    "                \n",
    "                self.res[i, :, :, j] = signal.convolve(self.x[i, :, :, :],\n",
    "                                                  np.rot90(self.kernels[j, :, :, :], 2),mode=\"valid\").reshape((W + 2*self.padW - self.kW + 1, H + 2*self.padH - self.kH + 1))[[i for i in range(0, W + 2*self.padW - self.kW + 1, self.Wstride)], :][:, [i for i in range(0, H + 2*self.padH - self.kH + 1, self.Hstride)]]\n",
    "        self.res += self.bias\n",
    "        self.res = self.Activation(self.res)\n",
    "        \n",
    "        return self.res\n",
    "    \n",
    "    \n",
    "    def backprop(self, optMethod, dE, alpha, momentum):\n",
    "        \n",
    "        dK = np.zeros_like(self.kernels)\n",
    "\n",
    "        for i in range(dE.shape[3]):\n",
    "            for j in range(self.x.shape[3]):\n",
    "                for k in range(self.x.shape[0]):\n",
    "\n",
    "                    dK[i, :, :, j] += signal.convolve(self.x[k, :, :, j], dE[k, :, :, i], mode=\"valid\") / self.x.shape[0]\n",
    "        \n",
    "        self.kernels = GD(alpha, self.kernels, dK)\n",
    "        self.bias = GD(alpha, self.bias, np.sum(dE, axis=0, keepdims=True) / dE.shape[0])\n",
    "            \n",
    "\n",
    "        new_dE = np.zeros_like(self.x)\n",
    "        \n",
    "        for i in range(dE.shape[0]):\n",
    "            for j in range(self.x.shape[3]):\n",
    "                for k in range(dE.shape[3]):\n",
    "                    \n",
    "                    new_dE[i, :, :, j] += signal.convolve(dE[i, :, :, k],\n",
    "                                                      np.rot90(self.kernels[k, :, :, j], 2), mode=\"full\") / dE.shape[3]\n",
    "        new_dE = new_dE[:, self.padW:self.x.shape[1]-self.padW, self.padH:self.x.shape[2]-self.padH, :]\n",
    "        return new_dE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "\n",
    "__Pooling Layers__ are layers that downsample their $W\\times{H}\\times{D}$ input matrices (activation maps) using a pooling operation (or strategy) by sliding a window (or pooling area) of shape $p_W\\times{p_H}$. The pooling operation is a __2D__ operation, meaning that the downsampling happens along the __Width__ and the __Height__ of the input matrix. Thus, the output matrix of the layer has the same depth as its input matrix. The most used pooling strategies are `max pooling` and `average pooling`. `Max pooling` extracts the maximum value from the moving pooling area, and `average pooling` calculates the average of the values in that pooling area and stores it in the output matrix. There are also some other pooling strategies like `min pooling` and `global pooling`, but we'll only use the first two in this notebook.\n",
    "\n",
    "<img src=\"imgs\\pooling.jpeg\" width='30%' height='30%' style=\"float:center\">\n",
    "<img src=\"imgs\\downsizing.png\" width='45%' height='45%' style=\"float:center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk more about the pooling area (or window). As previously said, the shape of this window is $p_W\\times{p_H}$ where $p_W < W_{in}$ and $p_H < H_{in}$, and most times $p_W=p_H$. Also, this window moves along the __width__ and the __height__ of a feature map with a desired step size. Let's call these step sizes $V_{step}$ and $H_{step}$ respectively (they are also called __strides__). These step sizes have the most influence on the output shape. Let's claculate the width and the height of the output matrix based on these parameters (V for Vertical and H for Horizontal):\n",
    "\n",
    "- $\\text{Output width}: \\ \\  W_{out} = \\lfloor\\frac{W_{in} - p_W}{V_{step}}\\rfloor + 1$\n",
    "\n",
    "\n",
    "- $\\text{Output height}: \\ \\  H_{out} = \\lfloor\\frac{H_{in} - p_H}{H_{step}}\\rfloor + 1$\n",
    "\n",
    "And the depth stays the same as before ($D_{out} = D_{in}$). Thus, this type of layer passes an output matrix (also called __Pooled Feature Map__) of shape $W_{out}\\times{H_{out}}\\times{D}$ to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "Pooling Layers have no paramaters such as weights or biases to optimize because they only downsample their inputs and pass them to the next layers. So during backpropagation, they upsample the given derivative matrix to match their input shape ($W_{in}\\times{H_{in}}\\times{D_{in}}$) and then pass it to the layers before them. The values stored in the upsampled matrix depend on the layer's pooling strategy. Suppose that the $i^{th}$ channel (depth) of the given derivative matrix is as followed:\n",
    "\n",
    "$$ \\mathscr{d}_i = \n",
    "\\begin{pmatrix}\n",
    "d_{11} & d_{12} & \\cdots & d_{1N} \\\\\n",
    "d_{21} & d_{22} & \\cdots & d_{2N}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "d_{M1} & d_{M2} & \\cdots & d_{MN}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Where $M = W_{out}$ and $N = H_{out}$.\n",
    "\n",
    "##### Max pooling\n",
    "\n",
    "First, we define a zero matrix of shape $W_{in}\\times{H_{in}}\\times{D_{in}}$. During the pooling seesion, for simplicity and as an example, assume that there is a pooling window of shape $2\\times{2}$ iterating over the $i^{th}$ channel of layer's input and the current values in that window are from rows {$r, r+1$} and columns {$c, c+1$}. These values are $a_{11}, a_{12}, a_{21}, a_{22}$, and $a_{21}$ is the maximum value. Thus, the window pools $a_{21}$ out and stores it in the layer's output matrix:\n",
    "\n",
    "$$max(\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}) = a_{21}\n",
    "$$\n",
    "\n",
    "$a_{21}$ is stored in (m, n, depth) position of the output matrix:\n",
    "\n",
    "- $m = \\lfloor\\frac{r+1 - p_W}{V_{step}}\\rfloor + 1 = \\lfloor\\frac{r+1 - 2}{V_{step}}\\rfloor + 1$\n",
    "- $n = \\lfloor\\frac{c+1 - p_H}{H_{step}}\\rfloor + 1 = \\lfloor\\frac{c+1 - 2}{H_{step}}\\rfloor + 1$\n",
    "- $depth = i$ (since $a_{21}$ is in the $i^{th}$ channel)\n",
    "\n",
    "\n",
    "Now, during backprop the derivative matrix for that specific area during the backprop would be $ \\ \\ \\frac{\\partial max(\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix})}{\\partial a_{ij}} = \\frac{\\partial{a_{21}}}{\\partial a_{ij}} = \\begin{bmatrix} 0 & 0 \\\\ 1 & 0\n",
    "\\end{bmatrix}\n",
    "$. Therefor, we find the original position of $a_{21}$ in the layer's input matrix and add the value $d_{mn}$ to that position inside the zero matrix we defined earlier. The rest are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average pooling\n",
    "\n",
    "For average pooling we have:\n",
    "$$\n",
    "mean(\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}) = \\frac{a_{11} + a_{12} + a_{21} + a_{22}}{4}\n",
    "\\\\ \\\\ \\\\\n",
    "\\frac{\\partial mean(\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix})}{\\partial a_{ij}} = \\begin{bmatrix} \\frac14 & \\frac14 \\\\ \\frac14 & \\frac14\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus, we find the original positions of $a_{11}, a_{12}, a_{21}, a_{22}$ in the layer's input matrix and add $\\frac14$ of the value $d_{mn}$ to those positions inside the zero matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class poolLayer:\n",
    "    \n",
    "    def __init__(self, pool_size=2, H_step_size=2, V_step_size=2, method=\"max\"):\n",
    "        \n",
    "        if pool_size < 1:\n",
    "            raise EnvironmentError(\"Poolling size should be a positive integer!!!\")\n",
    "            \n",
    "        if type(pool_size) != int:\n",
    "            raise EnvironmentError(\"Poolling size should be of type int!!!\")\n",
    "            \n",
    "        if H_step_size < 1:\n",
    "            raise EnvironmentError(\"Horizontal step size should be a positive integer!!!\")\n",
    "            \n",
    "        if type(H_step_size) != int:\n",
    "            raise EnvironmentError(\"Horizontal step size should be of type int!!!\")\n",
    "            \n",
    "        if V_step_size < 1:\n",
    "            raise EnvironmentError(\"Vertical step size should be a positive integer!!!\")\n",
    "            \n",
    "        if type(V_step_size) != int:\n",
    "            raise EnvironmentError(\"Vertical step size should be of type int!!!\")\n",
    "            \n",
    "        if method != \"max\" and method != \"avg\":\n",
    "            raise EnvironmentError(\"Poolling method not defined!!!\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        self.poolSize = pool_size\n",
    "        self.H_stepSize = H_step_size\n",
    "        self.V_stepSize = V_step_size\n",
    "        self.method = method\n",
    "        \n",
    "    def pool(self, x):\n",
    "        self.x =  x\n",
    "        n , W, H, D = x.shape\n",
    "        \n",
    "        self.W_out = int((W - self.poolSize) / self.H_stepSize + 1)\n",
    "        self.H_out = int((H - self.poolSize) / self.V_stepSize + 1)\n",
    "        self.D_out = D\n",
    "        \n",
    "        \n",
    "        res = np.zeros((n, self.W_out, self.H_out, D))\n",
    "        \n",
    "        if self.method == \"max\":\n",
    "            for i in range(self.poolSize, W, self.V_stepSize):\n",
    "                for j in range(self.poolSize, H, self.H_stepSize):\n",
    "                    N = (i-self.poolSize)//self.V_stepSize\n",
    "                    M = (j-self.poolSize)//self.H_stepSize\n",
    "                    res[:, N, M, :] = np.max( self.x[:, i-self.poolSize:i, j-self.poolSize:j, :], axis=(1, 2))\n",
    "        elif self.method == \"avg\":\n",
    "            for i in range(self.poolSize, W, self.V_stepSize):\n",
    "                for j in range(self.poolSize, H, self.H_stepSize):\n",
    "                    N = (i-self.poolSize)//self.V_stepSize\n",
    "                    M = (j-self.poolSize)//self.H_stepSize\n",
    "                    res[:, N, M, :] = np.mean( self.x[:, i-self.poolSize:i, j-self.poolSize:j, :], axis=(1, 2))\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def backprop(self, optMethod, dE, alpha, momentum):\n",
    "        \n",
    "        res = np.zeros_like(self.x)\n",
    "        _, W, H, _ = self.x.shape\n",
    "        \n",
    "        if self.method == \"max\":\n",
    "        \n",
    "            for i in range(self.x.shape[0]):\n",
    "                for j in range(self.D_out):\n",
    "                    for k in range(self.poolSize, W, self.V_stepSize):\n",
    "                        for l in range(self.poolSize, H, self.H_stepSize):\n",
    "\n",
    "                            indx = np.unravel_index(np.argmax(self.x[i, k-self.poolSize:k, l-self.poolSize:l, j]),\n",
    "                                                    self.x[i, k-self.poolSize:k, l-self.poolSize:l, j].shape)\n",
    "\n",
    "                            N = (k-self.poolSize)//self.V_stepSize\n",
    "                            M = (l-self.poolSize)//self.H_stepSize\n",
    "\n",
    "                            res[i, indx[0]+k-self.poolSize, indx[1]+l-self.poolSize, j] += dE[i, N, M, j]\n",
    "                            \n",
    "        elif self.method == \"avg\":\n",
    "            \n",
    "            for i in range(self.x.shape[0]):\n",
    "                for j in range(self.D_out):\n",
    "                    for k in range(self.poolSize, W, self.V_stepSize):\n",
    "                        for l in range(self.poolSize, H, self.H_stepSize):\n",
    "\n",
    "\n",
    "                            N = (k-self.poolSize)//self.V_stepSize\n",
    "                            M = (l-self.poolSize)//self.H_stepSize\n",
    "\n",
    "                            res[i, k-self.poolSize:k, l-self.poolSize:l, j] += dE[i, N, M, j]/self.poolSize**2\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNeuralNetwork:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 Layers=[convLayer(depth=3), poolLayer(), convLayer(depth=6), poolLayer(),\n",
    "                        FCLayer(n_neurons=10, Activation=\"soft\")], \n",
    "                 learning_rate=1e-3, opt=\"GD\", cost=\"SCE\",  num_iterations=15, momentum=.8, decay_rate = .95,\n",
    "                 loadPreTrainedModel=False, preTrainedModel=None):\n",
    "        \n",
    "        \n",
    "        if loadPreTrainedModel:\n",
    "            if type(preTrainedModel) == str:\n",
    "                \n",
    "                modelInfo = pickle.load(open(preTrainedModel, \"rb\"))\n",
    "                self.Layers = [None for i in range(len(modelInfo[\"Layers\"]))]\n",
    "                self.first_FCL_indx = None\n",
    "                \n",
    "                for i, layer in enumerate(modelInfo[\"Layers\"]):\n",
    "                    if modelInfo[layer][\"type\"] == \"CL\":\n",
    "                        \n",
    "                        self.Layers[i] = convLayer()\n",
    "                        \n",
    "                        self.Layers[i].kernels = modelInfo[layer][\"kernels\"]\n",
    "                        self.Layers[i].bias = modelInfo[layer][\"bias\"]\n",
    "                        if modelInfo[layer][\"activation\"] == \"ReLU\":\n",
    "                            self.Layers[i].Activation = lambda x: np.maximum(x, 0)\n",
    "                            self.Layers[i].activation = \"ReLU\"\n",
    "                        self.Layers[i].Wstride = modelInfo[layer][\"widthStride\"]\n",
    "                        self.Layers[i].Hstride = modelInfo[layer][\"heightStride\"]\n",
    "                        self.Layers[i].padW = modelInfo[layer][\"padWidth\"]\n",
    "                        self.Layers[i].padH = modelInfo[layer][\"padHeight\"]\n",
    "                        self.Layers[i].kW = self.Layers[i].kernels.shape[1]\n",
    "                        self.Layers[i].kH = self.Layers[i].kernels.shape[2]\n",
    "                        self.Layers[i].depth = self.Layers[i].kernels.shape[0]\n",
    "                        self.Layers[i].W_out = self.Layers[i].bias.shape[1]\n",
    "                        self.Layers[i].H_out = self.Layers[i].bias.shape[2]\n",
    "                        self.Layers[i].D_out = self.Layers[i].bias.shape[3]\n",
    "                        \n",
    "                    elif modelInfo[layer][\"type\"] == \"PL\":\n",
    "                        \n",
    "                        self.Layers[i] = poolLayer()\n",
    "                        \n",
    "                        self.Layers[i].poolSize = modelInfo[layer][\"poolSize\"]\n",
    "                        self.Layers[i].H_stepSize = modelInfo[layer][\"H_stepSize\"]\n",
    "                        self.Layers[i].V_stepSize = modelInfo[layer][\"V_stepSize\"]\n",
    "                        self.Layers[i].method = modelInfo[layer][\"method\"]\n",
    "                    \n",
    "                    elif modelInfo[layer][\"type\"] == \"FC\":\n",
    "                        if self.first_FCL_indx is None:\n",
    "                            self.first_FCL_indx = i\n",
    "                            \n",
    "                        self.Layers[i] = FCLayer(modelInfo[layer][\"weights\"].shape[0])\n",
    "                        \n",
    "                        self.Layers[i].W = modelInfo[layer][\"weights\"]\n",
    "                        self.Layers[i].b = modelInfo[layer][\"bias\"]\n",
    "                        self.Layers[i].activation = modelInfo[layer][\"Activation\"]\n",
    "                        self.Layers[i].Vw = modelInfo[layer][\"Vw\"]\n",
    "                        self.Layers[i].Vb = modelInfo[layer][\"Vb\"]\n",
    "                        self.Layers[i].W_cache = modelInfo[layer][\"W_cache\"]\n",
    "                        self.Layers[i].b_cache = modelInfo[layer][\"b_cache\"]\n",
    "                        self.Layers[i].shape = np.array([self.Layers[i].W.shape[0], self.Layers[i].W.shape[1]])\n",
    "                        \n",
    "                self.num_iterations =  modelInfo[\"iterations\"]\n",
    "                self.alpha = modelInfo[\"alpha\"]\n",
    "                self.optMethod = modelInfo[\"optMethod\"]\n",
    "                self.momentum = modelInfo[\"momentum\"]\n",
    "                self.decay_rate = modelInfo[\"decay_rate\"]\n",
    "                self.active = True\n",
    "                \n",
    "                if modelInfo[\"costFunc\"] == \"SCE\":\n",
    "                    self.costFunc = \"SCE\"\n",
    "                    self.cost = softmax_cross_entropy_cost\n",
    "                    self.dcost = dsoftmax_cross_entropy_cost\n",
    "                else:\n",
    "                    raise EnvironmentError(f\"There is no cost function such as {cost}\")\n",
    "                        \n",
    "                self.J = [] # train losses\n",
    "                self.val_J = [] # validation losses\n",
    "                \n",
    "                for i in range(self.first_FCL_indx, len(self.Layers)):\n",
    "                    if i == self.first_FCL_indx:\n",
    "                        self.Layers[i].activateLayer(modelInfo[\"FC1\"][\"weights\"].shape[1])\n",
    "                    else:\n",
    "                        self.Layers[i].activateLayer(self.Layers[i-1].n_neurons)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                raise EnvironmentError()\n",
    "                \n",
    "        else:\n",
    "        \n",
    "            # error handling\n",
    "            if len(Layers) == 0:\n",
    "                raise EnvironmentError(\"No layers to work with!!!\")\n",
    "\n",
    "            if type(Layers[-1]) != FCLayer:\n",
    "                raise EnvironmentError(\"The last layer should be a fully connected layer!!!\")\n",
    "\n",
    "            for i, layer in enumerate(Layers):\n",
    "\n",
    "                if i == 0 and type(layer) != convLayer:\n",
    "                    raise EnvironmentError(\"The first layer should be a convLayer!!!\")\n",
    "\n",
    "                if i != 0 and type(Layers[i]) == type(Layers[i-1]) and type(Layers[i]) == poolLayer:\n",
    "                    raise EnvironmentError(\"Two pooling layers should not come after eachother!!!\")\n",
    "\n",
    "                if i != 0 and (type(Layers[i]) == convLayer or type(Layers[i]) == poolLayer) and type(Layers[i-1]) == FCLayer:\n",
    "                    raise EnvironmentError(\"Fuly connected layers shoudl always come after pooling and conv layers!!!\")\n",
    "\n",
    "            self.Layers = Layers\n",
    "            self.num_iterations =  num_iterations\n",
    "            self.alpha = learning_rate\n",
    "            self.active = False\n",
    "            self.momentum = momentum\n",
    "            self.decay_rate = decay_rate\n",
    "\n",
    "            if opt != \"GD\" and opt != \"Momentum\" and opt != \"AdaGrad\" and opt !=\"RMSProp\" and opt !=\"Adam\":\n",
    "                raise EnvironmentError(\"optimization method not defined!!!\")\n",
    "                \n",
    "            self.optMethod = opt\n",
    "\n",
    "            \n",
    "            if cost == \"SCE\":\n",
    "                self.cost = softmax_cross_entropy_cost\n",
    "                self.dcost = dsoftmax_cross_entropy_cost\n",
    "            else:\n",
    "                raise EnvironmentError(f\"There is no cost function such as {cost}\")\n",
    "                \n",
    "            self.costFunc = cost\n",
    "\n",
    "            self.X_train = None # data that the model will be trained with\n",
    "            self.y_hat, self.y_train = None, None # predictions, correct classes    \n",
    "            self.J = [] # train losses\n",
    "            self.val_J = [] # validation losses\n",
    "\n",
    "            self.first_FCL_indx = None\n",
    "        \n",
    "    def fit(self, X_train, X_valid, y_train, y_valid, batch_size=12):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # error handling\n",
    "        if len(X_train.shape) != 4:\n",
    "            raise EnvironmentError(f\"X_train should be of shape (N, W, H, D)\")\n",
    "\n",
    "        if len(X_valid.shape) != 4:\n",
    "            raise EnvironmentError(f\"X_valid should be of shape (N, W, H, D)\")\n",
    "\n",
    "\n",
    "        # calculating the shape of the flatted output of the last conv or pool layer\n",
    "        # then activating the fully connected layers\n",
    "        \n",
    "        m, W, H, D = X_train.shape # m is the number of inputs\n",
    "        if not self.active :\n",
    "            for i, layer in enumerate(self.Layers):\n",
    "                if type(layer) == convLayer:\n",
    "                    W = int((W + 2*layer.padW - layer.kW) / layer.Wstride + 1)\n",
    "                    H = int((H + 2*layer.padH - layer.kH) / layer.Hstride + 1)\n",
    "                    D = layer.depth\n",
    "                elif type(layer) == poolLayer:\n",
    "                    W = int((W - layer.poolSize) / layer.H_stepSize + 1)\n",
    "                    H = int((H - layer.poolSize) / layer.V_stepSize + 1)\n",
    "                else:\n",
    "                    if type(self.Layers[i-1]) != FCLayer:\n",
    "                        layer.activateLayer(W*H*D)\n",
    "                    else:\n",
    "                        layer.activateLayer(self.Layers[i-1].n_neurons)\n",
    "                    if self.first_FCL_indx is None:\n",
    "                        self.first_FCL_indx = i\n",
    "            self.active = True\n",
    "        \n",
    "        # error handling            \n",
    "        if y_train.shape[0] != self.Layers[-1].shape[0]:\n",
    "            print(self.Layers[-1].shape, y_train.shape[0])\n",
    "            raise EnvironmentError(f\"y_train sholud be of shape({self.Layers[-1].shape[0]}, N)\")\n",
    "\n",
    "        if y_valid.shape[0] != self.Layers[-1].shape[0]:\n",
    "            raise EnvironmentError(f\"y_valid sholud be of shape({self.Layers[-1].shape[0]}, N)\")\n",
    "            \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        num_batches = m // batch_size\n",
    "        \n",
    "        report = \"Iteration {:3d}: training loss = {:.2f} | validation loss = {:.2f}\"\n",
    "        print(f\"{num_batches} batches\")\n",
    "        \n",
    "        for iteration in range(self.num_iterations):\n",
    "            train_loss = 0.\n",
    "            valid_loss = 0\n",
    "            for batch in range(num_batches):\n",
    "\n",
    "                # select a random mini-batch\n",
    "                idx = np.random.choice(m, batch_size, replace=False)\n",
    "                X_batch, y_batch = self.X_train[idx, :, :, :], self.y_train[:, idx]\n",
    "                \n",
    "                self.y_hat = self.predict(X_batch)\n",
    "                a = self.cost(self.y_hat, y_batch)\n",
    "                train_loss += a\n",
    "                self.update(X_batch, y_batch)\n",
    "                        \n",
    "                print(f\"iteration: {iteration}, batch: {batch}, loss: {a}\")\n",
    "\n",
    "            # report stats after each epoch\n",
    "            train_loss /= num_batches        \n",
    "            valid_loss = self.cost(self.predict(X_val), y_val)\n",
    "            \n",
    "            self.J.append(train_loss)\n",
    "            self.val_J.append(valid_loss)\n",
    "            print(report.format(iteration+1, train_loss, valid_loss))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        for i in range(len(self.Layers)):\n",
    "            if type(self.Layers[i]) == convLayer:\n",
    "                X = self.Layers[i].forward(X)\n",
    "            elif type(self.Layers[i]) == poolLayer:\n",
    "                X = self.Layers[i].pool(X)\n",
    "            else:\n",
    "                if len(X.shape) != 2:\n",
    "                    # flattening the results\n",
    "                    X = X.reshape(X.shape[0], -1)\n",
    "                    X = X.T\n",
    "                self.Layers[i].train(X)\n",
    "                X = self.Layers[i].a\n",
    "        # result of feed forward\n",
    "        return X\n",
    "    \n",
    "    # update function: optimizing to achieve optimal values for wieghts and biases\n",
    "    def update(self, X_batch, y_batch):\n",
    "        \n",
    "        size = X_batch.shape[0]\n",
    "        derivatives = []\n",
    "        # derivative of loss with respect to the result of the linear part in the networks last layer (z^n)\n",
    "        dJ = self.dcost(self.y_hat, y_batch) \n",
    "        derivatives.append(dJ / size)\n",
    "\n",
    "        for i in range(len(self.Layers) - 2, self.first_FCL_indx-1, -1):\n",
    "            # derivative of loss with respect to the result of the linear part in the ith layer\n",
    "            # in order to have the right shape, we take the dot product of W^T with dl/dz^(i+1)\n",
    "            # and multiply it (elementwise) with the result of this layers' activation part\n",
    "            derivative = (self.Layers[i+1].W.T @ derivatives[-1]) * self.Layers[i].dActivation(self.Layers[i].z)\n",
    "            derivatives.append(derivative)\n",
    "            \n",
    "        derivatives.reverse()\n",
    "        dE = 0\n",
    "\n",
    "        # computing for every dl/dW^i and dl/db^i\n",
    "        if self.optMethod == \"GD\":\n",
    "            for i in range(len(self.Layers) - 1, self.first_FCL_indx, -1):\n",
    "                \n",
    "                self.Layers[i].W = GD(self.alpha, self.Layers[i].W, \n",
    "                                       derivatives[i-self.first_FCL_indx] @ self.Layers[i-1].a.T)\n",
    "                \n",
    "                self.Layers[i].b = GD(self.alpha, self.Layers[i].b, \n",
    "                                       np.sum(derivatives[i-self.first_FCL_indx], axis=1, keepdims=True))\n",
    "\n",
    "            dE = self.Layers[self.first_FCL_indx].W.T @ derivatives[0]\n",
    "    \n",
    "            self.Layers[self.first_FCL_indx].W = GD(self.alpha, self.Layers[self.first_FCL_indx].W, \n",
    "                                       derivatives[0] @ self.Layers[self.first_FCL_indx].x.T)\n",
    "        \n",
    "            self.Layers[self.first_FCL_indx].b = GD(self.alpha, self.Layers[self.first_FCL_indx].b, \n",
    "                                       np.sum(derivatives[0], axis=1, keepdims=True))\n",
    "\n",
    "            \n",
    "        elif self.optMethod == \"Momentum\":\n",
    "            \n",
    "            for i in range(len(self.Layers) - 1, self.first_FCL_indx, -1):\n",
    "                self.Layers[i].Vw, self.Layers[i].W = Momentum(self.momentum, self.alpha, self.Layers[i].Vw,\n",
    "                                                               self.Layers[i].W, \n",
    "                                                               derivatives[i-self.first_FCL_indx] @ self.Layers[i-1].a.T)\n",
    "                \n",
    "                self.Layers[i].Vb, self.Layers[i].b = Momentum(self.momentum, self.alpha, self.Layers[i].Vb, \n",
    "                                                               self.Layers[i].b, \n",
    "                                                               np.sum(derivatives[i-self.first_FCL_indx], axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "            dE = self.Layers[self.first_FCL_indx].W.T @ derivatives[0]\n",
    "    \n",
    "            self.Layers[self.first_FCL_indx].Vw, self.Layers[self.first_FCL_indx].W = Momentum(self.momentum, self.alpha, self.Layers[self.first_FCL_indx].Vw,\n",
    "                                                               self.Layers[self.first_FCL_indx].W, \n",
    "                                                               derivatives[0] @ self.Layers[self.first_FCL_indx].x.T)\n",
    "                \n",
    "            self.Layers[self.first_FCL_indx].Vb, self.Layers[self.first_FCL_indx].b = Momentum(self.momentum, self.alpha, self.Layers[self.first_FCL_indx].Vb, \n",
    "                                                               self.Layers[self.first_FCL_indx].b, \n",
    "                                                               np.sum(derivatives[0], axis=1, keepdims=True))\n",
    "\n",
    "        elif self.optMethod == \"AdaGrad\":\n",
    "            for i in range(len(self.Layers) - 1, self.first_FCL_indx, -1):\n",
    "                self.Layers[i].W_cache, self.Layers[i].W = Adagrad(self.alpha, self.Layers[i].W,\n",
    "                                                                   derivatives[i-self.first_FCL_indx] @ self.Layers[i-1].a.T,\n",
    "                                                                   self.Layers[i].W_cache)\n",
    "                \n",
    "                self.Layers[i].b_cache, self.Layers[i].b = Adagrad(self.alpha, self.Layers[i].b,\n",
    "                                                                   np.sum(derivatives[i-self.first_FCL_indx], axis=1, keepdims=True),\n",
    "                                                                   self.Layers[i].b_cache)\n",
    "                \n",
    "            dE = self.Layers[self.first_FCL_indx].W.T @ derivatives[0]\n",
    "            \n",
    "            self.Layers[self.first_FCL_indx].W_cache, self.Layers[self.first_FCL_indx].W = Adagrad(self.alpha, \n",
    "                                                                   self.Layers[self.first_FCL_indx].W, derivatives[0] @ self.Layers[self.first_FCL_indx].x.T,\n",
    "                                                                   self.Layers[self.first_FCL_indx].W_cache)\n",
    "                \n",
    "            self.Layers[self.first_FCL_indx].b_cache, self.Layers[self.first_FCL_indx].b = Adagrad(self.alpha,\n",
    "                                                                   self.Layers[self.first_FCL_indx].b, np.sum(derivatives[0], axis=1, keepdims=True),\n",
    "                                                                   self.Layers[self.first_FCL_indx].b_cache)\n",
    "            \n",
    "        elif self.optMethod == \"RMSProp\":\n",
    "            for i in range(len(self.Layers) - 1, self.first_FCL_indx, -1):\n",
    "                self.Layers[i].W_cache, self.Layers[i].W = RMSProp(self.alpha, self.decay_rate, self.Layers[i].W,\n",
    "                                                                   derivatives[i-self.first_FCL_indx] @ self.Layers[i-1].a.T,\n",
    "                                                                   self.Layers[i].W_cache)\n",
    "                \n",
    "                self.Layers[i].b_cache, self.Layers[i].b = RMSProp(self.alpha, self.decay_rate, self.Layers[i].b,\n",
    "                                                                   np.sum(derivatives[i-self.first_FCL_indx], axis=1, keepdims=True),\n",
    "                                                                   self.Layers[i].b_cache)\n",
    "                \n",
    "            dE = self.Layers[self.first_FCL_indx].W.T @ derivatives[0]\n",
    "            \n",
    "            self.Layers[self.first_FCL_indx].W_cache, self.Layers[self.first_FCL_indx].W = RMSProp(self.alpha, self.decay_rate,\n",
    "                                                                   self.Layers[self.first_FCL_indx].W, derivatives[0] @ self.Layers[self.first_FCL_indx].x.T,\n",
    "                                                                   self.Layers[self.first_FCL_indx].W_cache)\n",
    "                \n",
    "            self.Layers[self.first_FCL_indx].b_cache, self.Layers[self.first_FCL_indx].b = RMSProp(self.alpha, self.decay_rate,\n",
    "                                                                   self.Layers[self.first_FCL_indx].b, np.sum(derivatives[0], axis=1, keepdims=True),\n",
    "                                                                   self.Layers[self.first_FCL_indx].b_cache)\n",
    "        \n",
    "        elif self.optMethod == \"Adam\":\n",
    "            for i in range(len(self.Layers) - 1, self.first_FCL_indx, -1):\n",
    "                self.Layers[i].W, self.Layers[i].Vw, self.Layers[i].W_cache = Adam(self.alpha, self.decay_rate, self.momentum,\n",
    "                                                                                  self.Layers[i].Vw, self.Layers[i].W,\n",
    "                                                                                  derivatives[i-self.first_FCL_indx] @ self.Layers[i-1].a.T, \n",
    "                                                                                  self.Layers[i].W_cache)\n",
    "                \n",
    "                self.Layers[i].b, self.Layers[i].Vb, self.Layers[i].b_cache = Adam(self.alpha, self.decay_rate, self.momentum,\n",
    "                                                                                  self.Layers[i].Vb, self.Layers[i].b,\n",
    "                                                                                  np.sum(derivatives[i-self.first_FCL_indx], axis=1, keepdims=True), \n",
    "                                                                                  self.Layers[i].b_cache)\n",
    "                \n",
    "            dE = self.Layers[self.first_FCL_indx].W.T @ derivatives[0]\n",
    "            \n",
    "            self.Layers[self.first_FCL_indx].W, self.Layers[self.first_FCL_indx].Vw, self.Layers[self.first_FCL_indx].W_cache = Adam(self.alpha, self.decay_rate, self.momentum,\n",
    "                                                                                  self.Layers[self.first_FCL_indx].Vw, self.Layers[self.first_FCL_indx].W,\n",
    "                                                                                  derivatives[0] @ self.Layers[self.first_FCL_indx].x.T, \n",
    "                                                                                  self.Layers[self.first_FCL_indx].W_cache)\n",
    "                \n",
    "            self.Layers[self.first_FCL_indx].b, self.Layers[self.first_FCL_indx].Vb, self.Layers[self.first_FCL_indx].b_cache = Adam(self.alpha, self.decay_rate, self.momentum,\n",
    "                                                                                  self.Layers[self.first_FCL_indx].Vb, self.Layers[self.first_FCL_indx].b,\n",
    "                                                                                  np.sum(derivatives[0], axis=1, keepdims=True), \n",
    "                                                                                  self.Layers[self.first_FCL_indx].b_cache)\n",
    "                \n",
    "                \n",
    "        \n",
    "        dE = dE.T.reshape((dE.shape[1], self.Layers[self.first_FCL_indx-1].W_out, self.Layers[self.first_FCL_indx-1].H_out,\n",
    "                               self.Layers[self.first_FCL_indx-1].D_out))\n",
    "        \n",
    "        for i in range(self.first_FCL_indx-1, -1, -1):\n",
    "            dE = self.Layers[i].backprop(self.optMethod, dE, self.alpha, self.momentum)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Hand Written Digit Classifier\n",
    "\n",
    "This model is consisted of two Convolution Layers and three Fully-Connected Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = ConvolutionNeuralNetwork(loadPreTrainedModel=True, preTrainedModel=\"HWDC_Model.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy =   96.19%\n",
      "Validation accuracy = 96.06%\n",
      "Testing accuracy = 96.20%\n"
     ]
    }
   ],
   "source": [
    "train_acc = accuracy(cnn.predict(X_train), y_train)\n",
    "valid_acc = accuracy(cnn.predict(X_val), y_val)\n",
    "test_acc = accuracy(cnn.predict(X_test), y_test)\n",
    "\n",
    "\n",
    "print('Training accuracy =   {:.2f}%'.format(train_acc))\n",
    "print('Validation accuracy = {:.2f}%'.format(valid_acc))\n",
    "print('Testing accuracy = {:.2f}%'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-8853fe8756b2>:6: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.subplot(5, 3, i+1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 15 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indx = random.randint(0, X_train.shape[0])\n",
    "r = cnn.Layers[0].forward(X_train[indx-1:indx])\n",
    "plt.figure(figsize=(6, 9))\n",
    "plt.subplot(5,3, 15)\n",
    "for i in range(r.shape[3]):\n",
    "    plt.subplot(5, 3, i+1)\n",
    "    plt.imshow(r[0, :, :, i], cmap=plt.cm.binary_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-fba6d48ada46>:5: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.subplot(4, 3, i+1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r1 = cnn.Layers[1].forward(r)\n",
    "plt.figure(figsize=(6, 9))\n",
    "plt.subplot(4,3, 12)\n",
    "for i in range(r1.shape[3]):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.imshow(r1[0, :, :, i], cmap=plt.cm.binary_r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
